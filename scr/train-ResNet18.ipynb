{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOVHlVcnPtiQXDE7PzK8VR3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Setup and imports\n"],"metadata":{"id":"Ops84_mgT9PO"}},{"cell_type":"markdown","source":["Install PyTorch, Torchvision, and tqdm into the Colab runtime (suppressing verbose output)."],"metadata":{"id":"q2U6erN6a9Qc"}},{"cell_type":"code","source":["!pip install torch torchvision tqdm --quiet"],"metadata":{"id":"J1rsyHCkUZFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import PyTorch core, neural-net layer (nn), optimzers, the dataloader, TorchVision utilities (datasets, transforms, model zoo), tqdm progress bars, Colab Drive API, Matplotlib for plotting, a function to refresh notebook output, and os for paths."],"metadata":{"id":"yhZDoCV5Ucmv"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","from tqdm import tqdm\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import os"],"metadata":{"id":"RS6DcowLUtsM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mount your Google Drive at `/content/drive` so reads/writes persist outside the ephemeral VM.\n","\n"],"metadata":{"id":"wtSPD54pUyKo"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"t48zxNcdVABG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Pick GPU if available; otherwise CPU. Print which one you're using."],"metadata":{"id":"CinVouR4VDEs"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using device:\", device)"],"metadata":{"id":"raWsKoOVT6Ma"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset paths and transforms\n","Define the folders where your training/validation images live (ImageFolder layout)."],"metadata":{"id":"s97l9MniVKEW"}},{"cell_type":"code","source":["train_dir = '/content/drive/MyDrive/dataset/train'\n","val_dir   = '/content/drive/MyDrive/dataset/val'"],"metadata":{"id":"1ABfBG-2VXYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the **training** preprocessing/augmentation pipeline:\n","\n","*   resize to ResNet's $224 \\times 224$,\n","*   random flips/rotations/color jitter for augmentation,\n","*   convert to tensor,\n","*   normalize with ImageNet mean/std (to match pretrained weights' expectations).\n","\n","\n","\n"],"metadata":{"id":"G-tpHKN0Vcmi"}},{"cell_type":"code","source":["train_tfms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"_soR2YYTVbpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build the **validation** pipeline: deterministic resize + normalization (no augmentation)."],"metadata":{"id":"Ok4szmHcWFp7"}},{"cell_type":"code","source":["val_tfms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"8fggsQ_eWBhW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create dataset objects that read subfolders class labels and apply transforms."],"metadata":{"id":"NTz8IDqtWS9P"}},{"cell_type":"code","source":["train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n","val_ds   = datasets.ImageFolder(val_dir, transform=val_tfms)"],"metadata":{"id":"VqE5DZ8DWSCZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Count classes from subfolder names and print them."],"metadata":{"id":"AmIej7bKWa5V"}},{"cell_type":"code","source":["num_classes = len(train_ds.classes)\n","print(f\"Detected {num_classes} classes:\", train_ds.classes)"],"metadata":{"id":"-QKVRPCNWaO4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wrap datasets into iterable mini-batches:\n","*   64 images per batch,\n","*   shuffle train, keep val deterministic,\n","* use 3 worker processes for background loading."],"metadata":{"id":"PJiU8oVTWhd6"}},{"cell_type":"code","source":["batch_size = 64\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"id":"N0GFCg_UWgrX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model setup (Transfer Learning)"],"metadata":{"id":"24qfEffRW3R_"}},{"cell_type":"markdown","source":["Load a ResNet-18 preinitialized on ImageNet (strong generic features)."],"metadata":{"id":"RIpyAFonW6rv"}},{"cell_type":"code","source":["model = models.resnet18(weights='IMAGENET1K_V1')"],"metadata":{"id":"loJ_MxNzXCXA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Freeze** all backbone parameters so only the classifier head trains (fast, avoids overfitting)."],"metadata":{"id":"H_w1CxT3XFJx"}},{"cell_type":"code","source":["for param in model.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"NrhQVeWFXLkd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Replace the final fully connected layer with a new classifier matching your class count."],"metadata":{"id":"uhRZkDCjXQ3h"}},{"cell_type":"code","source":["model.fc = nn.Linear(model.fc.in_features, num_classes)"],"metadata":{"id":"7MCmjBP1XQII"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Move model weights to GPU (or CPU) memory."],"metadata":{"id":"FIp01BaMXZ5b"}},{"cell_type":"code","source":["model = model.to(device)"],"metadata":{"id":"WZjUU1eTXZRD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the multi-class classification loss (negative log-likelihood over softmax)."],"metadata":{"id":"V4LCAvwYXf-t"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"tZvuwKlEXfKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimize only the new head's parameters using Adam at 1e-3 learning rate."],"metadata":{"id":"FeFH0E2sYL0v"}},{"cell_type":"code","source":["optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n","# After every 7 epochs, multiply the learnine rate by 0.1 (learning-rate decacy).\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\n","# Directory for checkpoints\n","ckpt_dir = '/content/drive/MyDrive/checkpoints'\n","os.makedirs(ckpt_dir, exist_ok=True)"],"metadata":{"id":"d5L4tTWTYEij"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"Z-jCjf1LYPDg"}},{"cell_type":"markdown","source":["\n","Train for 20 epochs and prepare lists to log metrics."],"metadata":{"id":"8OPj7sghYjM1"}},{"cell_type":"code","source":["num_epochs = 20\n","train_losses, val_losses, val_accuracies = [], [], []"],"metadata":{"id":"WIAEmf_OYlY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Start training!"],"metadata":{"id":"Wt-Vd2gSYxGa"}},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    # Switch to training mode (enables dropout/batchnorm updates).\n","    model.train()\n","\n","    # Zero the epoch loss accumulator.\n","    running_loss = 0.0\n","    # Create a progress bar over training batches.\n","    loop = tqdm(train_dl, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","    # Iterate over batches.\n","    for x, y in loop:\n","        # Move images/labels to GPU.\n","        x, y = x.to(device), y.to(device)\n","        # Clear gradients from the previous step.\n","        optimizer.zero_grad()\n","        # Forward pass: logits for the batch\n","        out = model(x)\n","\n","        # Compute cross-entropy loss against true labels.\n","        loss = criterion(out, y)\n","        # Backpropagate: compute gradients w.r.t. trainable parameteres (the head).\n","        loss.backward()\n","        # Take an optimizer step: update head weights.\n","        optimizer.step()\n","\n","        # Accumulate the sum of batch losses (scaled by batch size) for epoch mean.\n","        running_loss += loss.item() * x.size(0)\n","        # Show current batch loss in the tqdm bar.\n","        loop.set_postfix(loss=loss.item())\n","\n","    # Compute and store the mean training loss over the full epoch.\n","    epoch_train_loss = running_loss / len(train_dl.dataset)\n","    train_losses.append(epoch_train_loss)\n","\n","    # ----- Validation -----\n","    # Switch to eval mode (disables dropout; uses running stats).\n","    model.eval()\n","    # Init validation accumulators.\n","    val_loss, correct = 0.0, 0\n","\n","    # Validation loop\n","    with torch.no_grad():\n","        for x, y in val_dl:\n","            x, y = x.to(device), y.to(device)\n","            out = model(x)\n","            loss = criterion(out, y)\n","            val_loss += loss.item() * x.size(0)\n","            preds = out.argmax(1)\n","            correct += (preds == y).sum().item()\n","\n","    # Compute mean validation loss and accuracy; log them.\n","    epoch_val_loss = val_loss / len(val_dl.dataset)\n","    epoch_val_acc  = correct / len(val_dl.dataset)\n","    val_losses.append(epoch_val_loss)\n","    val_accuracies.append(epoch_val_acc)\n","\n","    # Update the learning rate per schedule (decays every 7 epochs).\n","    scheduler.step()\n","\n","    # ----- Live plot -----\n","    clear_output(wait=True)\n","    plt.figure(figsize=(8,4))\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Val Loss')\n","    plt.title(f\"Epoch {epoch+1}/{num_epochs} | Val Acc: {epoch_val_acc:.3f}\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n","          f\"Train Loss: {epoch_train_loss:.4f} | \"\n","          f\"Val Loss: {epoch_val_loss:.4f} | \"\n","          f\"Val Acc: {epoch_val_acc:.4f}\")\n","\n","    # ----- Save checkpoint -----\n","    ckpt_path = f\"{ckpt_dir}/resnet18_epoch_{epoch+1}.pth\"\n","    torch.save(model.state_dict(), ckpt_path)\n","    print(f\"âœ… Saved checkpoint: {ckpt_path}\")"],"metadata":{"id":"egTEWqzXYtyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Post-training summary plots"],"metadata":{"id":"KQpejzG9asFN"}},{"cell_type":"code","source":["plt.figure(figsize=(8,4))\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(val_losses, label='Val Loss')\n","plt.title(\"Training vs Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","plt.figure(figsize=(8,4))\n","plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n","plt.title(\"Validation Accuracy over Epochs\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","print(\"ðŸŽ¯ Training complete. Best model stored in:\", ckpt_dir)"],"metadata":{"id":"GYjMpHvz5OZu"},"execution_count":null,"outputs":[]}]}